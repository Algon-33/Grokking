{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d503d322",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Grokking papers\n",
    "[Omnigrok]( https://arxiv.org/abs/2210.01117)$\\newline$\n",
    "[Impartial Games](https://arxiv.org/abs/2205.12787)$\\newline$\n",
    "[Grokking phase transitions]( https://arxiv.org/abs/2210.15435)$\\newline$\n",
    "[Mechanistic Interpretability Analysis of Grokking](https://www.lesswrong.com/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking)$\\newline$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37515ca7-1ebe-4df3-87fd-4fe19281f6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ca9740b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Is Grokking Even Possible in Deep RL?\n",
    "\n",
    "The problem with grokking in MDPs is that typically there is one unique policy for each reward function. \n",
    "\n",
    "Now this might not be true if there is some state in which every action results in the same transition probabilities. Or if there are some symmetries amongst the states. \n",
    "\n",
    "OK, I was forgetting my theorems. There is one optimal VALUE FUNCTION but not one optimal policy for a given MDP. Different MDPS may share the same value functions. \n",
    "\n",
    "Hmm, it still feels kind of funny though. Like, who really checks to see if deep RL generalizes or whatever? Alright, that question has an obvious answer.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e18ae5",
   "metadata": {},
   "source": [
    "### Literature Search For Deep RL Grokking\n",
    "I checked for grokking and that didn't turn anything worthwhile up.\n",
    "\n",
    "Then I checked for phase transitions, and that was mostly physics stuff. \n",
    "\n",
    "After thinking about how grokking would be possible/show up in deep RL, \n",
    "I'm now checking for generelization and OOD robustness algorithms, to see if those display qualatative behaviour similair to grokking, or can be manipulated to do so in a similair way to what tegmark wrote about.\n",
    "\n",
    "$\\newline$\n",
    "\n",
    "Here is a benchmark for DeepRL: \n",
    "[Assessing Generalization in Deep RL 2018](https://ar5iv.labs.arxiv.org/html/1810.12282)\n",
    "They find vanilla RL algorithms work better than custom ones designed to help generelization. So I guess they show how not to grok, and how not to generalize.\n",
    "\n",
    "$\\newline$\n",
    "\n",
    "\n",
    "[Quantifying Generelization in DeepRL 2018](https://proceedings.mlr.press/v97/cobbe19a.html)\n",
    "They make a new environment coinrun as a benchmark for generelization. Agents overfit to suprisingly large datasets. Deeper networks, L2 regularization, dropout, data augmentation and batch norm aid generalization. \n",
    "\n",
    "Sounds good.\n",
    "$\\newline$\n",
    "\n",
    "\n",
    "https://www.semanticscholar.org/paper/Quantifying-Generalization-in-Reinforcement-Cobbe-Klimov/ef2bc452812d6005ab0a66af6c3f97b6b0ba837e?sort=is-influential\n",
    "\n",
    "[A Study on Overfitting in Deep Reinforcement Learning 2018](https://arxiv.org/abs/1804.06893)\n",
    "Common generalization techniques with stochasticity don't prevent/detect overfitting. Agents and learning algorithms can have drastically different test performance.\n",
    "\n",
    "$\\newline$\n",
    "\n",
    "\n",
    "[Measuring and Characterizing Generalization in deep RL 2018](https://onlinelibrary.wiley.com/doi/full/10.1002/ail2.45)\n",
    "Proposes definitions for deepRL, methods for checking for those defs. DQNs make poor decisions for states differing only slightly from on-policy states. \n",
    "\n",
    "That's sort of interesting. I wonder if decision transformers, or whatever the ones which learn from expert trajectories, also fail to generalize? Maybe their networks are too small.\n",
    "$\\newline$\n",
    "\n",
    "\n",
    "\n",
    "[A simple Randomization Technique for Generalization in DRL 2019](https://ar5iv.labs.arxiv.org/html/1910.05396)\n",
    "They randomly perturb input observations in their CNN. They use inference methods based on monte carlo approximation to reduce variance. They outperform various regularization/data augmentations methods on 2D coinrun, 3d deepmind lab exploration and 3d robotic control tasks. \n",
    "\n",
    "Kind of interesting. I guess you might try checking if vanilla methods will eventually grok whatever representations this thing has. So, perhaps, you switch around lower level inputs nearby each other vs not and that is testing for whether or not the method learns lower level concepts more or less reliably. Likewise for inputs to deeper parts of the network. Maybe if you assume that once it knows the high level concepts, which should have some symmetry, it will learn a symmetric algorithm more easily than for lower level inputs?\n",
    "\n",
    "\n",
    "$\\large{NOTE}$: Think about how a method will impact stability of network, variance of gradients/outputs. That seems a useful thing to check. More important yet is having a model in your head for their ideas to latch onto.\n",
    "\n",
    "$\\newline$\n",
    "\n",
    "\n",
    "\n",
    "[Illuminating Generalization in Deep Reinforcement Learning through Procedural Level Generation](https://arxiv.org/abs/1806.10729)\n",
    "\n",
    "\n",
    "\n",
    "[Self Supervised DRL with Generalized Comptation Graphs for Robot Navigation](https://ieeexplore.ieee.org/abstract/document/8460655)\n",
    "\n",
    "Does model based RL have different implications for grokking than model free? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb30654d-a7cd-4f26-b1f5-4ee558d92536",
   "metadata": {},
   "source": [
    "#### [Quantifying Generelization in DeepRL 2018](https://proceedings.mlr.press/v97/cobbe19a.html)\n",
    "\n",
    "I am going to focus on this paper. \n",
    "\n",
    "They talk about developing a new benchmark, coinrun. That is accesible via the [OpenAI procgen evnrionment](https://openai.com/blog/procgen-benchmark/). There's also an implementation of PPO trained on this benchmark [here](https://github.com/jbkjr/train-procgen-pytorch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca72bba-6c89-40a6-9f1c-3af92996acec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
